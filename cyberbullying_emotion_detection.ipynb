{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e7652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Srushti\n",
      "[nltk_data]     Rawal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "                                          tweet_text cyberbullying_type\n",
      "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
      "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
      "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
      "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
      "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
      "Text cleaning completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff5c2a2a1834e60afefcff0f339ce93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Srushti Rawal\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Srushti Rawal\\.cache\\huggingface\\hub\\models--j-hartmann--emotion-english-distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fd3a9de35f4ddfa815d7260b4999d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb5e8540e71435ca91462ce2d1653a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: j-hartmann/emotion-english-distilroberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71488d09e94141fd935adee271995046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03fc6cdf61a4ea5919463f2c2237dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665689077cd94823b1daca7cd9dfb735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a1769c5f874e43a474c90a47d9f6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888555b3e4d34f43b0c6ab2396af9043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd95fd4c3b411c93107310ddf59c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion classification model loaded\n",
      "Emotion detection started (this may take time)...\n",
      "Emotion column added successfully\n",
      "Final dataset saved as cyberbullying_with_emotion.xlsx\n",
      "\n",
      "Emotion Distribution:\n",
      "Emotion\n",
      "anger       16423\n",
      "neutral     10374\n",
      "fear         6302\n",
      "sadness      5013\n",
      "joy          4176\n",
      "surprise     3041\n",
      "disgust      2363\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# cyberbullying_emotion_detection.py\n",
    "\n",
    "# ===============================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "\n",
    "# ===============================\n",
    "# 2. DOWNLOAD REQUIRED NLTK DATA\n",
    "# ===============================\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ===============================\n",
    "# 3. LOAD DATASET\n",
    "# ===============================\n",
    "# Ensure CBTweets.csv is in the same directory\n",
    "df = pd.read_csv(\"CBTweets.csv\")\n",
    "\n",
    "print(\"Dataset loaded successfully\")\n",
    "print(df.head())\n",
    "\n",
    "# ===============================\n",
    "# 4. TEXT CLEANING FUNCTION\n",
    "# ===============================\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)      # remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)          # remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)          # remove hashtags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)      # remove special characters\n",
    "\n",
    "    tokens = text.split()                     # SAFE tokenization\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply text cleaning\n",
    "df[\"clean_text\"] = df[\"tweet_text\"].apply(clean_text)\n",
    "\n",
    "print(\"Text cleaning completed\")\n",
    "\n",
    "# ===============================\n",
    "# 5. LOAD EMOTION CLASSIFIER\n",
    "# ===============================\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    return_all_scores=False\n",
    ")\n",
    "\n",
    "print(\"Emotion classification model loaded\")\n",
    "\n",
    "# ===============================\n",
    "# 6. EMOTION DETECTION FUNCTION\n",
    "# ===============================\n",
    "def detect_emotion(text):\n",
    "    try:\n",
    "        result = emotion_classifier(text[:512])\n",
    "        return result[0]['label']\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# ===============================\n",
    "# 7. CREATE EMOTION COLUMN\n",
    "# ===============================\n",
    "print(\"Emotion detection started (this may take time)...\")\n",
    "\n",
    "df[\"Emotion\"] = df[\"clean_text\"].apply(detect_emotion)\n",
    "\n",
    "print(\"Emotion column added successfully\")\n",
    "\n",
    "# ===============================\n",
    "# 8. SAVE FINAL DATASET\n",
    "# ===============================\n",
    "output_file = \"cyberbullying_with_emotion.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Final dataset saved as {output_file}\")\n",
    "\n",
    "# ===============================\n",
    "# 9. QUICK SUMMARY\n",
    "# ===============================\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "print(df[\"Emotion\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
